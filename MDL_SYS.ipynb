{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv1D, Dropout,Input,BatchNormalization,MaxPooling1D,concatenate\n",
    "# from tensorflow.keras.utils import np_utils,multi_gpu_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as bek\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6,7\"\n",
    "# Specify the GPUs you want to use\n",
    "# visible_devices = ['GPU:0', 'GPU:1']  # Adjust indices as needed\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data cell\n",
    "\"\"\"\n",
    "df1=pd.read_csv('/home/dp/data/Bit_old/Bit_BP_new/datapath/pair_end_merge_SYS_random_400_split_up.csv',\n",
    "                    index_col = 0)\n",
    "\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(df1['SYS']):\n",
    "    if(df1['time'].iloc[i]==2):\n",
    "        continue\n",
    "    elif(df1['time'].iloc[i]==1):\n",
    "        temp=df1[(df1['Person No']==df1['Person No'].iloc[i])&(df1['time']==2)]['SYS']\n",
    "        if(temp.empty):\n",
    "            continue\n",
    "        else:\n",
    "            #print(df1['SYS'].iloc[i],temp.iloc[0])\n",
    "            df1['SYS'].iloc[i]=temp.iloc[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2 = df1['count']!=0\n",
    "df1 = df1[cond2]\n",
    "\n",
    "target = 'SYS'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['SD_last']=df1['SYS_last']-df1['DIA_last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model cell\n",
    "'''\n",
    "def base_model():\n",
    "    in_s = Input((406, 2))\n",
    "    model_s = Conv1D(filters=256, kernel_size=3,  name=\"block1_conv\") (in_s)\n",
    "    model_s = BatchNormalization(axis=-1,  name=\"block1_batchnorm\")(model_s)\n",
    "    model_s = Activation('relu', name=\"block1_act\")(model_s)\n",
    "    model_s = MaxPooling1D(pool_size=2, name=\"block1_pool\")(model_s)\n",
    "    \n",
    "    model_s = Conv1D(filters=256, kernel_size=3, name=\"block2_conv\") (model_s)\n",
    "    model_s = BatchNormalization(axis=-1,  name=\"block2_batchnorm\")(model_s)\n",
    "    model_s = Activation('relu', name=\"block2_act\")(model_s)\n",
    "    model_s = MaxPooling1D(pool_size=2, name=\"block2_pool\")(model_s)\n",
    "    \n",
    "    model_s = Conv1D(filters=512, kernel_size=3, name=\"block3_conv\") (model_s)\n",
    "    model_s = BatchNormalization(axis=-1,  name=\"block3_batchnorm\")(model_s)\n",
    "    model_s = Activation('relu', name=\"block3_act\")(model_s)\n",
    "    model_s = MaxPooling1D(pool_size=2, name=\"block3_pool\")(model_s)\n",
    "    \n",
    "    model_s = Conv1D(filters=1024, kernel_size=3, name=\"block4_conv\") (model_s)\n",
    "    model_s = BatchNormalization(axis=-1,  name=\"block4_batchnorm\")(model_s)\n",
    "    model_s = Activation('relu', name=\"block4_act\")(model_s)\n",
    "    model_s = MaxPooling1D(pool_size=2, name=\"block4pool\")(model_s)\n",
    "    \n",
    "    model_s = Conv1D(filters=2048, kernel_size=3, name=\"block5_conv\") (model_s)\n",
    "    model_s = BatchNormalization(axis=-1,  name=\"block5_batchnorm\")(model_s)\n",
    "    model_s = Activation('relu', name=\"block5_act\")(model_s)\n",
    "    model_s = MaxPooling1D(pool_size=2, name=\"block5_pool\")(model_s)\n",
    "    \n",
    "    model_s = Flatten(name=\"flatten\")(model_s)\n",
    "    \n",
    "    input1 = keras.layers.Input(shape=(2,), name= \"dl_input\")\n",
    "    c = keras.layers.Concatenate(axis=-1)([model_s, input1])\n",
    "    \n",
    "    m = BatchNormalization(axis=-1, name = \"batch_norm\")(c)\n",
    "    m = Dense(4096,activation='relu', name= 'FC1')(m)\n",
    "    m = Dense(2048,activation='relu', name= 'FC2')(m)\n",
    "    out = Dense(1,activation='linear', name=\"predictions\")(m)\n",
    "    \n",
    "    model = Model(inputs=[in_s,input1], outputs=[out])\n",
    "\n",
    "    # Compile the model\n",
    "    adam=keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(train_temp,test_temp):\n",
    "    X_train_s = train_temp.iloc[:,17:423]\n",
    "    X_train_s_last = train_temp.iloc[:,425:831]\n",
    "    X_test_s = test_temp.iloc[:,17:423]\n",
    "    X_test_s_last = test_temp.iloc[:,425:831]\n",
    "    \n",
    "    f1=['SYS']\n",
    "    Y_train = train_temp[f1]\n",
    "    Y_train = scalert.transform(Y_train)\n",
    "    \n",
    "    # reshape train data\n",
    "    X_train_r_s = np.zeros((len(X_train_s), 406, 2))\n",
    "    X_train_r_s[:, :, 0] = X_train_s\n",
    "    X_train_r_s[:, :, 1] = X_train_s_last\n",
    "\n",
    "    # reshape test data\n",
    "    X_test_r_s = np.zeros((len(X_test_s), 406, 2))\n",
    "    X_test_r_s[:, :, 0] = X_test_s\n",
    "    X_test_r_s[:, :, 1] = X_test_s_last\n",
    "    \n",
    "    f2=['SYS_last','SD_last']\n",
    "    # reshape train data\n",
    "    X_train_r_f = train_temp[f2]\n",
    "\n",
    "    # reshape test data\n",
    "    X_test_r_f = test_temp[f2]\n",
    "    \n",
    "    #fit\n",
    "    filepath=\"SYS_weights_best-Copy1.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', save_best_only=True,mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    # Create a MirroredStrategy.\n",
    "    tf.debugging.set_log_device_placement(False)\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    print(\"Number of devices: {}\".format(mirrored_strategy.num_replicas_in_sync)) \n",
    "    \n",
    "    with mirrored_strategy.scope():\n",
    "        # Define your model\n",
    "        model = base_model()  \n",
    "    \n",
    "    \n",
    "    Y_train = Y_train.reshape((Y_train.shape[0],))\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((X_train_r_s, X_train_r_f), Y_train))\n",
    "    #train_dataset = train_dataset.batch(3000)\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    train_dataset = train_dataset.with_options(options)\n",
    "    \n",
    "    # seting it, https://stackoverflow.com/questions/49922252/choosing-number-of-steps-per-epoch\n",
    "    steps_per_Epoch = int( np.ceil(X_train_r_s.shape[0] / 3000) )\n",
    "    history = model.fit(train_dataset.batch(batch_size = 3000).repeat(),  epochs = 750,  steps_per_epoch = steps_per_Epoch, callbacks=callbacks_list)\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.ylim(0,0.2)\n",
    "    plt.show()\n",
    "    \n",
    "    test_pred = model.predict([X_test_r_s,X_test_r_f])\n",
    "    test_pred1 = scalert.inverse_transform(test_pred)\n",
    "    test_temp['SYS_p']=test_pred1\n",
    "    \n",
    "    temp_a=0\n",
    "    temp_b=0\n",
    "    test_SYS_r=[]\n",
    "    test_SYS_p=[]\n",
    "\n",
    "    \n",
    "    for i,item in enumerate(test_temp['Person No']):\n",
    "        if((test_temp['Person No'].iloc[i]!=temp_a) | (test_temp['time'].iloc[i]!=temp_b)):\n",
    "            test_SYS_r.append(test_temp['SYS'].iloc[i])\n",
    "            temp = test_temp[(test_temp['Person No']==test_temp['Person No'].iloc[i])&(test_temp['time']==test_temp['time'].iloc[i])]\n",
    "            t = np.median(temp['SYS_p'])\n",
    "            test_SYS_p.append(t)\n",
    "            temp_a = test_temp['Person No'].iloc[i]\n",
    "            temp_b = test_temp['time'].iloc[i]\n",
    "\n",
    "            \n",
    "    #print(test_SYS_p)\n",
    "    #print(test_SYS_r)\n",
    "    del model\n",
    "    bek.clear_session()\n",
    "    gc.collect()\n",
    "    return (test_SYS_p,test_SYS_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step1(train_temp,test_temp):\n",
    "    X_train_s = train_temp.iloc[:,17:423]\n",
    "    X_train_s_last = train_temp.iloc[:,425:831]\n",
    "    X_test_s = test_temp.iloc[:,17:423]\n",
    "    X_test_s_last = test_temp.iloc[:,425:831]\n",
    "    \n",
    "    f1=['SYS']\n",
    "    Y_train = train_temp[f1]\n",
    "    Y_train = scalert.transform(Y_train)\n",
    "    \n",
    "   # reshape train data\n",
    "    X_train_r_s = np.zeros((len(X_train_s), 406, 2))\n",
    "    X_train_r_s[:, :, 0] = X_train_s\n",
    "    X_train_r_s[:, :, 1] = X_train_s_last\n",
    "\n",
    "    # reshap validation data\n",
    "    X_test_r_s = np.zeros((len(X_test_s), 406, 2))\n",
    "    X_test_r_s[:, :, 0] = X_test_s\n",
    "    X_test_r_s[:, :, 1] = X_test_s_last\n",
    "    \n",
    "    f2=['SYS_last',\"SD_last\"]\n",
    "    # reshape train data\n",
    "    X_train_r_f = train_temp[f2]\n",
    "\n",
    "    # reshape validation data\n",
    "    X_test_r_f = test_temp[f2]\n",
    "    \n",
    "     # Create a MirroredStrategy.\n",
    "    tf.debugging.set_log_device_placement(False)\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        # Define your model\n",
    "        model1 = base_model()  \n",
    "\n",
    "    model1.load_weights(\"SYS_weights_best-Copy1.hdf5\")\n",
    "                        \n",
    "    test_pred = model1.predict([X_test_r_s,X_test_r_f])\n",
    "    test_pred1 = scalert.inverse_transform(test_pred)\n",
    "    test_temp['SYS_p']=test_pred1\n",
    "    \n",
    "    temp_a=0\n",
    "    temp_b=0\n",
    "    test_SYS_r=[]\n",
    "    test_SYS_p=[]\n",
    "\n",
    "    \n",
    "    for i,item in enumerate(test_temp['Person No']):\n",
    "        if((test_temp['Person No'].iloc[i]!=temp_a) | (test_temp['time'].iloc[i]!=temp_b)):\n",
    "            \n",
    "            #administrator mode\n",
    "            test_SYS_r.append(test_temp['SYS'].iloc[i])\n",
    "            temp = test_temp[(test_temp['Person No']==test_temp['Person No'].iloc[i])&(test_temp['time']==test_temp['time'].iloc[i])]\n",
    "            t = np.median(temp['SYS_p'])\n",
    "            test_SYS_p.append(t)\n",
    "            temp_a = test_temp['Person No'].iloc[i]\n",
    "            temp_b = test_temp['time'].iloc[i]\n",
    "\n",
    "    \n",
    "    del model1\n",
    "    bek.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    return (test_SYS_p,test_SYS_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=9\n",
    "\n",
    "savedir_results = 'SYS/Personalized_pair_of_'\n",
    "dir_name = f'{savedir_results}_P{n:01d}'\n",
    "if os.path.exists(dir_name):\n",
    "    dir_name = dir_name\n",
    "else:\n",
    "    os.makedirs(dir_name)\n",
    "    \n",
    "#testingNo_all = [1273,543,1002,342,245,149,1888,1373,1020,1775,1598, 1436,2095,2096,1722]\n",
    "testingNo_all = [342,149]\n",
    "for testingNo in testingNo_all:\n",
    "    print('###########',testingNo,'###########')\n",
    "    \n",
    "    test_p=[]\n",
    "    test_r=[]\n",
    "    test_p_best=[]\n",
    "    test_r_best=[]\n",
    "    \n",
    "    train = df1\n",
    "    leftout_data = df1[(df1['count']==testingNo)&(df1['count_time']>(n))]\n",
    "    leftout_data_index = leftout_data.index\n",
    "    train = train.drop(leftout_data_index)\n",
    "    \n",
    "    t = train[target]\n",
    "    t = t.values.reshape(-1, 1)\n",
    "    scalert = preprocessing.MinMaxScaler().fit(t)\n",
    "    \n",
    "    test  = df1[(df1['count']==testingNo)&(df1['count_time']>=10)]        \n",
    "\n",
    "    (temp_pred,temp_real) = test_step(train,test)\n",
    "    (temp_pred_best,temp_real_best) = test_step1(train,test)\n",
    "\n",
    "    test_p=[temp_pred[i:i + 2] for i in range(0, len(temp_pred), 2)]\n",
    "    test_r=[temp_real[i:i + 2] for i in range(0, len(temp_real), 2)]\n",
    "\n",
    "    test_p_best=[temp_pred_best[i:i + 2] for i in range(0, len(temp_pred_best), 2)]\n",
    "    test_r_best=[temp_real_best[i:i + 2] for i in range(0, len(temp_real_best), 2)]\n",
    "        \n",
    "    d = {'SYS_real': test_r, 'SYS_pred': test_p}\n",
    "    df_f = pd.DataFrame(data = d)\n",
    "\n",
    "    ind =[]\n",
    "    for j in range(2,len(df_f.index)+2):\n",
    "        ind.append(j)\n",
    "\n",
    "    df_f.index=ind\n",
    "\n",
    "    name_temp =str(testingNo)+'-1000-256.csv'\n",
    "    df_f.to_csv(dir_name + '/' + name_temp)\n",
    "\n",
    "\n",
    "    d_best = {'SYS_real': test_r_best, 'SYS_pred': test_p_best}\n",
    "    df_f_best = pd.DataFrame(data = d_best)\n",
    "\n",
    "    ind =[]\n",
    "    for j in range(2,len(df_f_best.index)+2):\n",
    "        ind.append(j)\n",
    "\n",
    "    df_f_best.index=ind\n",
    "\n",
    "    name_temp_best =str(testingNo)+'-1000-256-best.csv'\n",
    "    df_f_best.to_csv(dir_name + '/' + name_temp_best)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
